
<!DOCTYPE html>

<html>

<head>
   <style>
      td, th {
        border: 0px solid black;          
        }
      img{
   padding: 5px;
}
      </style>

  <title>Labelfree OVIS</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  
  <link rel="shortcut icon" href="./static/images/ovis/jhu_web.png" />

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.css">
  <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

<script src="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

</head>

<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Open-Vocabulary Instance Segmentation without Manual Mask Annotations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vibashan.github.io/">Vibashan VS</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ningyu1991.github.io/">Ning Yu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=tAUdLM0AAAAJ&hl=en">Chen Xing</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://canqin.tech/">Can Qin</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://fly6464.github.io/">Mingfei Gao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="http://www.niebles.net/">Juan Carlos Niebles</a><sup>2</sup>,
            </span>
            <span class="author-block"></span>
              <a href="https://engineering.jhu.edu/vpatel36/sciencex_teams/vishalpatel/">Vishal M. Patel,</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=sgBB2sUAAAAJ&hl=en">Ran Xu</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Johns Hopkins University<sup>1</sup>,</span>
            <span class="author-block">Northeastern University<sup>3</sup>,</span>
            <span class="author-block">Salesforce Research<sup>2</sup></span>
          </div>
         
         <div class="column has-text-centered">
            <a href="as">CVPR 2023</a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a href="https://drive.google.com/file/d/1j9-1oOzh89T6_wy-aAXBriLNlvh80D60/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary material</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://github.com/Vibashan/Labelfree-OVIS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Colab</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3">Overview</h2>
        <img src="./static/images/ovis/ovis-gif5.gif" alt="" border=0 height=600 width=1500></img>
        <div class="content has-text-justified">
          <ul>
            <li> Our work proposes a manual-mask-free approach for open-vocabulary instance segmentation leveraging a pre-trained vision-language model. </li>
            <li> Specifically, we generate pseudo-masks annotations for objects of interest from image-caption pairs using pre-trained VLM and weakly-supervised proposal and segmentation network. </li>
            <li> These generated pseduo-mask annotations are then used to train an instance segmentation model, completely eliminating the need for human-provided box-level or pixel-level annotations. </li>

          </ul>
          
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pseduo-mask Generation Framework</h2>
          <div class="content has-text-justified">
            <h5 class="subtitle has-text-centered"></h5> 

          <center>Overview of the proposed framework</center>
            <img src="./static/images/ovis/Archi_v2.svg" alt="" border=0 height=500 width=1500></img></
          <p>
            Given an image-caption pair, we generate a GradCAM activation for the object of interest, leveraging the localization capability of a pre-trained vision-language model (in this case "umbrella").
             To further refine the activation map, we perform an iterative masking strategy. Then, using the activation map as a guidance function, we choose the best proposals that cover the object from a 
             weakly-supervised proposal network (WSPN) and generate box-level annotations. Next, we crop the image based on the generated pseudo-bounding box and perform weakly-supervised segmentation to obtain 
             pixel-level annotations. Overall, given an image-caption pair, our pipeline generates pseudo-mask annotations by leveraging the pre-trained vision-language and weakly supervised models. 
             Finally, the generated pseudo-mask annotations are used to train an instance segmentation model, eliminating the need for human-provided box-level or pixel-level annotations.
          </p> 

      
          </div>
       </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pseduo-mask visualizations</h2>
          <div class="content has-text-justified">
            <h5 class="subtitle has-text-centered"></h5> 
            <img src="./static/images/ovis/qual_vis.png" alt="" border=0 height=500 width=1500></img></
          <p>
            (A) Visualization of activation maps generated to cover the object of interest (woman and dog), which were used to select the best bounding box proposal.
            (B) Visualization of generated pseudo-mask annotations generated using our pipeline.
          </p> 

      
          </div>
       </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Mask-rcnn predicition visualizations</h2>
          <div class="content has-text-justified">
            <h5 class="subtitle has-text-centered"></h5> 
            <img src="./static/images/ovis/pred-vis.svg" alt="" border=0 height=500 width=1500></img></
          <p>
            The top and bottom rows of the visualization display the predictions generated by the Mask-RCNN model, which was trained on pseudo-masks created from two datasets: COCO and Open Images, repectively.
            By training on a large number of pseudo-masks, the Mask-RCNN model is able to filter out any noise present in the masks, leading to improved predictions that include complete masks and tight bounding boxes.
          </p> 

      
          </div>
       </div>
      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container content is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre><code>
  @article{vs2023open,
     title={Open-Vocabulary Instance Segmentation without Manual Mask Annotations},
     author={VS, Vibashan and Yu, Ning and Xing, Chen and Qin, Can and Gao, Mingfei and Niebles, Juan Carlos and Patel, Vishal M and Xu, Ran},
     journal={arXiv preprint arXiv},
     year={2023}
   }</code></pre>
  </div>
</section>

<section class="section" >
  <div class="container is-max-desktop content">
    <h5 class="title"> Acknowledgement: The website Template taken from <span class="author-block">
              <a href="https://nerfies.github.io/" target="_blank">Nerfies</a></h5>

  </div>
</section>

<script>
    const viewers = document.querySelectorAll(".image-compare");
    viewers.forEach((element) => {
        let view = new ImageCompare(element, {
            hoverStart: true,
            addCircle: true
        }).mount();
    });

    $(document).ready(function () {
        var editor = CodeMirror.fromTextArea(document.getElementById("bibtex"), {
            lineNumbers: false,
            lineWrapping: true,
            readOnly: true
        });
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    });
</script>
</body>
</html>
